\subsection{Bias Discussion}

The term "bias" has many different scientific meanings, and at least two distinct definitions in forecasting terminology:

%TODO define in brief for the glossary
\gls{bias}

\gls{bias-correction}


\begin{enumerate}
\item Flat application across 
\item TODO find notes on this! argh
\end{enumerate}

Bias is a common source of error in forecasting, and can be difficult to conceptualize due to the differences in applications of bias across disciplines. A classic optimistic bias is the tendency to overemphasize rare events through the "representativeness heuristic" (\autocite{kahneman2011thinking}). 

In forecasting sciences, however, an optimistic bias may develop  during (forecast) model development: if the data set used during model setup is used for verification, the resulting biased score reflects "artificial skill". To avoid measuring artificial skill the model developer should break their data set into two representative series: a \textit{training set} and a \textit{test set}. \autocite{JolliffeIanT.andStephenson2012ForecastVerification}

\gls{bias-correction} is another aspect of meteorological and hydrological forecasting, and is computed similarly to bias analyses in the cognitive sciences noted above.

A simple example to illustrate some challenges in detecting - much less controlling - bias follows.

In a contingency table - or a statistical hypothesis test ("did something happen, or not?"), there are four states: two "correct" inferences and two errors. A Type I Error is a false positive; a Type II Error is a discarded negative ("false negative") hypothesis.

\begin{table}[ht]
\centering
\caption{Type I and Type II errors in hypothesis (H$_{\text{0}}$) testing}
\label{tbl:type-i-ii-errors}
\begin{tabular}{r|p{5cm}|p{4cm}|p{4cm}|}
    \hline
     & H $_{\text{0}}$ = TRUE & H $_{\text{0}}$ = FALSE\\
    \hline
    H $_{\text{0}}$ not rejected & Correct & Type II error\\
    \hline
    H $_{\text{0}}$ rejected & Type I error & Correct\\
    \hline
\end{tabular}
\end{table}

Using our tornado example, if a twister was forecast \textbf{and} occurred, that's a validation of the hypothesis (H $_{\text{0}}$ = TRUE) and is counted in the upper left box; the lower left box is twisters that were predicted but either 1) not observed or 2) never materialized.

The right column (H $_{\text{0}}$ = FALSE) is more complicated. Up top, no tornadoes were forecast - but one (or more) happened. On the bottom, "non-twisters" are counted that didn't happen when they were forecast to not happen ("Correct"). 

Because tornadoes -- like flash floods -- are rare events, getting a reliable value for tornadoes \textbf{not} forecast and\textbf{ not} happening is hard, and this value generally has the highest uncertainty of the table.

If the data are reliable, however, bias in a contingency table (or dichotomous) forecast is a simple calculation:
\begin{equation}\label{BiasEquation}
bias =  \frac{ hits + false alarms }{ hits + misses }
\end{equation}


TODO
Bias correction methods in common use for hydrology include the six following from  ~\autocite{teutschbein2013bias}.

\begin{table}[ht]
\centering
\caption{Comparison of bias correction methods}
\label{tbl:compare-bias-correction}
\begin{tabular}{lllll} %{|p{2cm}|pp{2cm}|pp{2cm}|pp{2cm}|pp{2cm}|}
linear transformations & Precipitation &  &  &  \\
LOCI                   & Precipitation &  &  &  \\
power transformation   &               &  &  &  \\
variance scaling       &               &  &  & \\
distribution mapping   &               &  &  & \\
delta-change approach  &               &  &  & 
\end{tabular}
\end{table}


Specific to CRPS scores, a method exists for de-biasing CRPS scores \autocite{ferro2008effect}. TODO

Piani \autocite{Piani2010StatisticalEurope} points out that bias correction is ... 